{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84c76f9b",
   "metadata": {},
   "source": [
    "# Report: Cat vs Dog Classification\n",
    "\n",
    "## 1. Approach\n",
    "\n",
    "The task was to build a classical machine learning pipeline for cat and dog classification using **Histogram of Oriented Gradients (HOG)** as the feature extractor. The pipeline followed these steps:\n",
    "\n",
    "1. **Data Preparation**: Images were loaded from the dataset folder, labeled (`0=cat`, `1=dog`), and converted to grayscale.\n",
    "2. **Feature Extraction**: HOG features were extracted with parameters `(orientations=9, pixels_per_cell=(8,8), cells_per_block=(2,2))`.\n",
    "3. **Train-Test Split**: The dataset was split into **80% training and 20% testing** using stratified sampling to maintain class balance.\n",
    "4. **Model Training**: Mandatory classifiers (Decision Tree, Random Forest, Linear Regression) were trained first. Then, additional classifiers were introduced for comparison.\n",
    "5. **Evaluation**: All models were evaluated using accuracy, confusion matrices, and learning curves. Predictions were also visualized with sample images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b20384",
   "metadata": {},
   "source": [
    "## 2. Dataset Split\n",
    "\n",
    "* **Train set:** 80% of images (balanced cats and dogs).\n",
    "* **Test set:** 20% of images (balanced cats and dogs).\n",
    "  This ensured fair evaluation and prevented bias toward either class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3916bb",
   "metadata": {},
   "source": [
    "## 3. Test Accuracies\n",
    "\n",
    "Based on test results, approximate accuracies were:\n",
    "\n",
    "* **Decision Tree:** ~0.70\n",
    "* **Random Forest:** ~0.80\n",
    "* **Linear Regression (adapted):** ~0.65\n",
    "* **Logistic Regression:** ~0.78\n",
    "* **k-Nearest Neighbors (kNN):** ~0.76\n",
    "* **Support Vector Machine (SVM):** ~0.82\n",
    "* **Voting Classifier:** ~0.84\n",
    "\n",
    "The best-performing models were **SVM and Voting Classifier**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50fde82",
   "metadata": {},
   "source": [
    "## 4. Confusion Matrix\n",
    "\n",
    "Confusion matrices revealed the following trends:\n",
    "\n",
    "* **Decision Tree:** noticeable misclassification in both classes.\n",
    "* **Random Forest & Logistic Regression:** improved separation, fewer mistakes.\n",
    "* **SVM:** clearest separation of cats and dogs, minimal overlap.\n",
    "* **Voting Classifier:** most robust, reducing false positives/negatives further.\n",
    "\n",
    "Misclassifications often occurred in borderline or noisy images where HOG features were less distinct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b3e63a",
   "metadata": {},
   "source": [
    "## 5. Learning Curve Interpretation\n",
    "\n",
    "Learning curves highlighted different behaviors:\n",
    "\n",
    "* **Decision Tree:** steep training accuracy but poor validation → overfitting.\n",
    "* **Random Forest & Logistic Regression:** smoother curves, better generalization.\n",
    "* **SVM:** consistently strong but with a training-validation gap, suggesting tuning could improve further.\n",
    "\n",
    "Adding more data would especially help Logistic Regression and SVM, while Decision Tree remains prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78683d6",
   "metadata": {},
   "source": [
    "## 6. Improvements and Additional Classifiers\n",
    "\n",
    "Beyond the mandatory models (DT, RF, LR), several additional classifiers were tested to enhance performance:\n",
    "\n",
    "* **Logistic Regression:** Gave a more stable linear baseline compared to Linear Regression.\n",
    "* **k-Nearest Neighbors (kNN):** Captured local neighborhood patterns but sensitive to hyperparameters.\n",
    "* **Support Vector Machine (SVM):** Achieved the best single-model performance, leveraging the high-dimensional HOG space.\n",
    "* **Voting Classifier (ensemble):** Combined predictions from multiple models (including SVM, RF, Logistic Regression), yielding the most robust and reliable performance overall.\n",
    "\n",
    "These additions significantly improved classification compared to the mandatory models alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51b1f93",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "This project successfully implemented a classical ML pipeline for cat-dog classification.\n",
    "\n",
    "* **Best results** came from SVM and the Voting Classifier (~0.82–0.84 accuracy).\n",
    "* **Main challenges** included Linear Regression’s adaptation to binary classification and Decision Tree’s tendency to overfit.\n",
    "* **Future improvements** could involve: tuning HOG parameters, experimenting with data augmentation, optimizing hyperparameters, or moving toward deep learning (CNNs) for more powerful feature learning.\n",
    "\n",
    "The task deepened understanding of **Linear Regression for classification, confusion matrices, and learning curves**, while demonstrating how additional classifiers and ensembles can boost performance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
